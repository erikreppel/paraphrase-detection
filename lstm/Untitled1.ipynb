{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 32\n",
    "n_examples = 200\n",
    "sequence_len = 30  # This is equivalent to time steps of the sequence in keras\n",
    "sequence_size = 1\n",
    "hidden_size = 50\n",
    "nb_layers = 2\n",
    "target_size = 1\n",
    "\n",
    "# Generate noisy sine-wave\n",
    "# When I want to complexify (many to one), just swap the x and y\n",
    "NSAMPLE = 100000\n",
    "f = 2 # the frequency of the signal\n",
    "x_data = np.float32(np.arange(NSAMPLE))\n",
    "r_data = np.float32(np.random.uniform(-0.2, 0.2, NSAMPLE))\n",
    "y_data = np.float32(np.sin(2 * np.pi * f* (x_data/NSAMPLE)) + r_data)\n",
    "# Build the training data\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, y_data.shape[0], sequence_len):\n",
    "    if i+sequence_len < y_data.shape[0]:\n",
    "        X.append(x_data[i:i+sequence_len])\n",
    "        y.append(y_data[i+sequence_len]) # next point\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class my_rnn_1(nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=200, num_layers=3, output_size=1,\n",
    "                 batch_size=10):\n",
    "        super(my_rnn_1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size,\n",
    "                          num_layers=self.num_layers, batch_first=True)\n",
    "        # The last layer is applied on the last output only of the RNN (not like\n",
    "        # TimeDistributedDense in Keras)\n",
    "        self.linear_layer = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input_sequence, hidden):\n",
    "        out_rnn, hidden = self.rnn(input_sequence, hidden)\n",
    "        in_linear = out_rnn[:, -1, :]\n",
    "        final_output = self.linear_layer(in_linear)\n",
    "        return final_output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros((self.num_layers, self.batch_size, self.hidden_size))).cuda()\n",
    "\n",
    "\n",
    "def get_batch(X, Y, i, evaluation=False):\n",
    "    global batch_size\n",
    "    seq_len = min(batch_size, len(X) - 1 - i)\n",
    "    data = X[i:i+seq_len, :]\n",
    "    data = data.view(data.size(0), data.size(1), 1)\n",
    "    target = Y[i+1:i+1+seq_len].view(-1, 1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  0.021808099001646042\n",
      "Loss =  21.75482982955873\n",
      "Loss =  21.83757818862796\n",
      "Loss =  21.907587070018053\n",
      "Loss =  21.99952644854784\n",
      "Loss =  22.106887688860297\n",
      "Loss =  22.217913476750255\n",
      "Loss =  22.323810967616737\n",
      "Loss =  22.396161892451346\n",
      "Loss =  22.471900936216116\n",
      "Loss =  22.567031182348728\n",
      "Loss =  22.68315613642335\n",
      "Loss =  22.771494332700968\n",
      "Loss =  22.842347744852304\n",
      "Loss =  22.946574336849153\n",
      "Loss =  23.034023942425847\n",
      "Loss =  23.142377232201397\n",
      "Loss =  23.218812914565206\n",
      "Loss =  23.3036791337654\n",
      "Loss =  23.385761943645775\n",
      "Loss =  23.482355510815978\n",
      "Epoch 0 -- loss = 23.539296231232584\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.026830799877643585\n",
      "Loss =  0.11231172177940607\n",
      "Loss =  0.1995922140777111\n",
      "Loss =  0.2811197768896818\n",
      "Loss =  0.43221230059862137\n",
      "Loss =  0.6962181031703949\n",
      "Loss =  1.246470533311367\n",
      "Loss =  1.7153039649128914\n",
      "Loss =  2.187428407371044\n",
      "Loss =  2.8447896614670753\n",
      "Loss =  3.215068232268095\n",
      "Loss =  3.4547212990000844\n",
      "Loss =  3.5710489274933934\n",
      "Loss =  3.6401112750172615\n",
      "Loss =  3.955625556409359\n",
      "Loss =  4.877074755728245\n",
      "Loss =  21.2267412468791\n",
      "Loss =  22.212970534339547\n",
      "Loss =  22.288754703477025\n",
      "Loss =  22.366216874681413\n",
      "Loss =  22.45292399544269\n",
      "Epoch 1 -- loss = 22.52844846341759\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.03411939740180969\n",
      "Loss =  0.12916001956909895\n",
      "Loss =  0.20886211562901735\n",
      "Loss =  0.28140822798013687\n",
      "Loss =  0.37054867949336767\n",
      "Loss =  0.47097968962043524\n",
      "Loss =  0.5768689280375838\n",
      "Loss =  0.6837495490908623\n",
      "Loss =  0.7572331028059125\n",
      "Loss =  0.8322147186845541\n",
      "Loss =  0.9248972740024328\n",
      "Loss =  1.035726873204112\n",
      "Loss =  1.122896222397685\n",
      "Loss =  1.197706300765276\n",
      "Loss =  1.3162870090454817\n",
      "Loss =  1.4206667477265\n",
      "Loss =  1.53939220122993\n",
      "Loss =  1.612964628264308\n",
      "Loss =  1.7185478135943413\n",
      "Loss =  1.7944780439138412\n",
      "Loss =  1.9076549094170332\n",
      "Epoch 2 -- loss = 1.9625825881958008\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.03961052745580673\n",
      "Loss =  0.2080146325752139\n",
      "Loss =  0.550975195132196\n",
      "Loss =  0.8885261295363307\n",
      "Loss =  1.3313021464273334\n",
      "Loss =  1.861908278428018\n",
      "Loss =  2.3338944679126143\n",
      "Loss =  3.0091599663719535\n",
      "Loss =  3.46193456556648\n",
      "Loss =  3.639308131299913\n",
      "Loss =  3.8718991400673985\n",
      "Loss =  4.222694422118366\n",
      "Loss =  4.8504202319309115\n",
      "Loss =  5.665489370934665\n",
      "Loss =  5.910081724636257\n",
      "Loss =  6.002275303937495\n",
      "Loss =  6.095043059438467\n",
      "Loss =  6.168093003332615\n",
      "Loss =  6.2867527306079865\n",
      "Loss =  6.406046103686094\n",
      "Loss =  6.504281112924218\n",
      "Epoch 3 -- loss = 6.741624163463712\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.0600828155875206\n",
      "Loss =  1.132876455783844\n",
      "Loss =  2.619205042719841\n",
      "Loss =  3.48161680996418\n",
      "Loss =  3.731914956122637\n",
      "Loss =  3.8560984656214714\n",
      "Loss =  3.9744238322600722\n",
      "Loss =  4.078265079297125\n",
      "Loss =  4.165634559467435\n",
      "Loss =  4.2361687906086445\n",
      "Loss =  4.329880226403475\n",
      "Loss =  4.500002784654498\n",
      "Loss =  5.031260279938579\n",
      "Loss =  6.254131240770221\n",
      "Loss =  6.792757065966725\n",
      "Loss =  6.962956354022026\n",
      "Loss =  7.072802955284715\n",
      "Loss =  7.195831844583154\n",
      "Loss =  7.259029594250023\n",
      "Loss =  7.4468622310087085\n",
      "Loss =  8.363494674675167\n",
      "Epoch 4 -- loss = 8.999865437857807\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.350462406873703\n",
      "Loss =  1.2543662041425705\n",
      "Loss =  1.5592404212802649\n",
      "Loss =  1.6315743485465646\n",
      "Loss =  1.7205964056774974\n",
      "Loss =  1.8233829131349921\n",
      "Loss =  1.9696725402027369\n",
      "Loss =  2.0770577136427164\n",
      "Loss =  2.1800817474722862\n",
      "Loss =  2.614328321069479\n",
      "Loss =  3.464294832199812\n",
      "Loss =  4.416140276938677\n",
      "Loss =  4.887058328837156\n",
      "Loss =  5.0377123560756445\n",
      "Loss =  5.362832119688392\n",
      "Loss =  5.738305313512683\n",
      "Loss =  6.116514725610614\n",
      "Loss =  6.316269749775529\n",
      "Loss =  6.671529797837138\n",
      "Loss =  6.840701434761286\n",
      "Loss =  6.917202448472381\n",
      "Epoch 5 -- loss = 7.016605230048299\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.012999623082578182\n",
      "Loss =  0.11196914594620466\n",
      "Loss =  0.30307812336832285\n",
      "Loss =  1.8548502707853913\n",
      "Loss =  3.7870549941435456\n",
      "Loss =  4.370983864180744\n",
      "Loss =  4.585862190462649\n",
      "Loss =  4.685189496725798\n",
      "Loss =  4.761086542159319\n",
      "Loss =  4.8439003033563495\n",
      "Loss =  4.926613908261061\n",
      "Loss =  5.038926910609007\n",
      "Loss =  5.242583563551307\n",
      "Loss =  5.847682369872928\n",
      "Loss =  6.251571929082274\n",
      "Loss =  6.484090324491262\n",
      "Loss =  6.6710035260766745\n",
      "Loss =  6.967323748394847\n",
      "Loss =  7.190930740907788\n",
      "Loss =  7.951932014897466\n",
      "Loss =  8.996401892974973\n",
      "Epoch 6 -- loss = 9.295222828164697\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.1619037389755249\n",
      "Loss =  0.5869666785001755\n",
      "Loss =  0.8462127074599266\n",
      "Loss =  0.9263196839019656\n",
      "Loss =  1.00932474154979\n",
      "Loss =  1.1115891383960843\n",
      "Loss =  1.2885254295542836\n",
      "Loss =  1.4405254293233156\n",
      "Loss =  1.6843932922929525\n",
      "Loss =  2.6854305621236563\n",
      "Loss =  3.9917810168117285\n",
      "Loss =  4.680369475856423\n",
      "Loss =  4.86127900518477\n",
      "Loss =  4.937327515333891\n",
      "Loss =  5.207210201770067\n",
      "Loss =  5.66234877333045\n",
      "Loss =  6.152275796979666\n",
      "Loss =  6.407591558992863\n",
      "Loss =  6.789922267198563\n",
      "Loss =  6.955353923141956\n",
      "Loss =  7.0301428670063615\n",
      "Epoch 7 -- loss = 7.12308116722852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.013026563450694084\n",
      "Loss =  0.09532794542610645\n",
      "Loss =  0.1781799988821149\n",
      "Loss =  0.5713373413309455\n",
      "Loss =  2.183596326969564\n",
      "Loss =  3.7718298276886344\n",
      "Loss =  4.237606416456401\n",
      "Loss =  4.34726822655648\n",
      "Loss =  4.423157445155084\n",
      "Loss =  4.5115638487041\n",
      "Loss =  4.595172563567758\n",
      "Loss =  4.708274846896529\n",
      "Loss =  4.902169929817319\n",
      "Loss =  5.4686411041766405\n",
      "Loss =  5.948214055970311\n",
      "Loss =  6.3339123744517565\n",
      "Loss =  6.684220368042588\n",
      "Loss =  7.137826154008508\n",
      "Loss =  7.353392401710153\n",
      "Loss =  7.787162961438298\n",
      "Loss =  8.463755192235112\n",
      "Epoch 8 -- loss = 8.768532173708081\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.19290387630462646\n",
      "Loss =  0.8278154656291008\n",
      "Loss =  1.1874810680747032\n",
      "Loss =  1.2861488042399287\n",
      "Loss =  1.3669517179951072\n",
      "Loss =  1.4550431929528713\n",
      "Loss =  1.5932679809629917\n",
      "Loss =  1.712343206629157\n",
      "Loss =  1.887553141452372\n",
      "Loss =  2.8034049132838845\n",
      "Loss =  4.255741850472987\n",
      "Loss =  4.985552714206278\n",
      "Loss =  5.1662632236257195\n",
      "Loss =  5.238459552638233\n",
      "Loss =  5.471738182939589\n",
      "Loss =  5.863254827447236\n",
      "Loss =  6.40585230011493\n",
      "Loss =  6.7807491617277265\n",
      "Loss =  7.363064636476338\n",
      "Loss =  7.6170368464663625\n",
      "Loss =  7.715934884734452\n",
      "Epoch 9 -- loss = 7.840625044889748\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.013745740056037903\n",
      "Loss =  0.12394959572702646\n",
      "Loss =  0.24622717406600714\n",
      "Loss =  0.7922148397192359\n",
      "Loss =  2.0160042038187385\n",
      "Loss =  2.936699896119535\n",
      "Loss =  3.4138735784217715\n",
      "Loss =  3.5502453604713082\n",
      "Loss =  3.6347021842375398\n",
      "Loss =  3.7551536979153752\n",
      "Loss =  3.850197171792388\n",
      "Loss =  3.95334686152637\n",
      "Loss =  4.04029555618763\n",
      "Loss =  4.279338911175728\n",
      "Loss =  4.6322378776967525\n",
      "Loss =  5.3817584328353405\n",
      "Loss =  6.668437089771032\n",
      "Loss =  7.661750238388777\n",
      "Loss =  7.8357270415872335\n",
      "Loss =  8.033993816003203\n",
      "Loss =  8.272177692502737\n",
      "Epoch 10 -- loss = 8.34206423163414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.05448627471923828\n",
      "Loss =  0.24436888843774796\n",
      "Loss =  0.5149267837405205\n",
      "Loss =  0.6816563401371241\n",
      "Loss =  0.8446968123316765\n",
      "Loss =  1.0807549487799406\n",
      "Loss =  1.5110397022217512\n",
      "Loss =  2.8409953247755766\n",
      "Loss =  3.837504519149661\n",
      "Loss =  4.101887525990605\n",
      "Loss =  4.324439434334636\n",
      "Loss =  4.5418380638584495\n",
      "Loss =  4.797694104723632\n",
      "Loss =  5.136892749927938\n",
      "Loss =  5.265962100587785\n",
      "Loss =  5.3411814868450165\n",
      "Loss =  5.462680554017425\n",
      "Loss =  5.591089569032192\n",
      "Loss =  6.360136039555073\n",
      "Loss =  7.507160298526287\n",
      "Loss =  8.129316806793213\n",
      "Epoch 11 -- loss = 8.489815853536129\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.030842069536447525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  0.2171129733324051\n",
      "Loss =  0.3266754485666752\n",
      "Loss =  0.5639862567186356\n",
      "Loss =  0.9728029780089855\n",
      "Loss =  1.5526726208627224\n",
      "Loss =  2.216251101344824\n",
      "Loss =  2.50916600599885\n",
      "Loss =  2.6993693597614765\n",
      "Loss =  2.9997731745243073\n",
      "Loss =  3.2339473105967045\n",
      "Loss =  3.4740934148430824\n",
      "Loss =  3.6572127044200897\n",
      "Loss =  3.8333043325692415\n",
      "Loss =  4.8143255058676\n",
      "Loss =  5.996345380321145\n",
      "Loss =  6.6784595269709826\n",
      "Loss =  6.876622445881367\n",
      "Loss =  7.107344675809145\n",
      "Loss =  7.1962796887382865\n",
      "Loss =  7.274148617871106\n",
      "Epoch 12 -- loss = 7.328880170360208\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.025061480700969696\n",
      "Loss =  0.13070367462933064\n",
      "Loss =  0.3511202968657017\n",
      "Loss =  0.6086595095694065\n",
      "Loss =  1.1399539969861507\n",
      "Loss =  2.2180050425231457\n",
      "Loss =  3.048817526549101\n",
      "Loss =  3.691783893853426\n",
      "Loss =  3.9601178243756294\n",
      "Loss =  4.055501759052277\n",
      "Loss =  4.201794940978289\n",
      "Loss =  4.423586176708341\n",
      "Loss =  4.828363360837102\n",
      "Loss =  5.522247465327382\n",
      "Loss =  5.819723412394524\n",
      "Loss =  5.953937036916614\n",
      "Loss =  6.064417767338455\n",
      "Loss =  6.212674955837429\n",
      "Loss =  6.303442246280611\n",
      "Loss =  6.759762073867023\n",
      "Loss =  8.305120314471424\n",
      "Epoch 13 -- loss = 8.916901673190296\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.2591417133808136\n",
      "Loss =  0.806274339556694\n",
      "Loss =  1.0302289947867393\n",
      "Loss =  1.1014761542901397\n",
      "Loss =  1.19249411765486\n",
      "Loss =  1.3059817282482982\n",
      "Loss =  1.4859208008274436\n",
      "Loss =  1.6161058517172933\n",
      "Loss =  1.7583772903308272\n",
      "Loss =  2.3004434471949935\n",
      "Loss =  3.2873016661033034\n",
      "Loss =  4.157758100889623\n",
      "Loss =  4.505061297677457\n",
      "Loss =  4.6397821037098765\n",
      "Loss =  5.0294976672157645\n",
      "Loss =  5.596953704021871\n",
      "Loss =  6.173928405158222\n",
      "Loss =  6.461247417144477\n",
      "Loss =  6.86876994650811\n",
      "Loss =  7.0399752808734775\n",
      "Loss =  7.115610749460757\n",
      "Epoch 14 -- loss = 7.209859629161656\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.012999565340578556\n",
      "Loss =  0.09631262067705393\n",
      "Loss =  0.1792881963774562\n",
      "Loss =  0.542584384791553\n",
      "Loss =  2.0012427968904376\n",
      "Loss =  3.382423589937389\n",
      "Loss =  3.8488222947344184\n",
      "Loss =  3.9629374984651804\n",
      "Loss =  4.039478433318436\n",
      "Loss =  4.133025793358684\n",
      "Loss =  4.2160743698477745\n",
      "Loss =  4.322350004687905\n",
      "Loss =  4.485085966065526\n",
      "Loss =  4.978490965440869\n",
      "Loss =  5.456126229837537\n",
      "Loss =  5.9234191458672285\n",
      "Loss =  6.436784764751792\n",
      "Loss =  7.132482884451747\n",
      "Loss =  7.423418940976262\n",
      "Loss =  7.901824535802007\n",
      "Loss =  8.544318655505776\n",
      "Epoch 15 -- loss = 8.734093414619565\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.12171970307826996\n",
      "Loss =  0.48838307335972786\n",
      "Loss =  0.7947183512151241\n",
      "Loss =  0.9080200213938951\n",
      "Loss =  0.993634489364922\n",
      "Loss =  1.0763815827667713\n",
      "Loss =  1.1759647075086832\n",
      "Loss =  1.2695483155548573\n",
      "Loss =  1.3814248647540808\n",
      "Loss =  1.6136924102902412\n",
      "Loss =  3.4372680708765984\n",
      "Loss =  5.420387141406536\n",
      "Loss =  5.867075636982918\n",
      "Loss =  6.017556874081492\n",
      "Loss =  6.123047420755029\n",
      "Loss =  6.247591010294855\n",
      "Loss =  6.407214964739978\n",
      "Loss =  6.499442688189447\n",
      "Loss =  6.713621887378395\n",
      "Loss =  6.852057985030115\n",
      "Loss =  6.930549785494804\n",
      "Epoch 16 -- loss = 7.049053950235248\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.015427660197019577\n",
      "Loss =  0.19198668841272593\n",
      "Loss =  0.6470029884949327\n",
      "Loss =  2.416983329690993\n",
      "Loss =  3.6088885152712464\n",
      "Loss =  3.934278425760567\n",
      "Loss =  4.1025504330173135\n",
      "Loss =  4.2018821109086275\n",
      "Loss =  4.278991467319429\n",
      "Loss =  4.359058799222112\n",
      "Loss =  4.441862570121884\n",
      "Loss =  4.557927990332246\n",
      "Loss =  4.805033255368471\n",
      "Loss =  5.6555499099195\n",
      "Loss =  6.360992286354303\n",
      "Loss =  6.763428378850222\n",
      "Loss =  7.0259416326880455\n",
      "Loss =  7.314869288355112\n",
      "Loss =  7.443661030381918\n",
      "Loss =  7.817811157554388\n",
      "Loss =  8.736606147140265\n",
      "Epoch 17 -- loss = 9.128932807594538\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.19897721707820892\n",
      "Loss =  0.7645315080881119\n",
      "Loss =  1.1056175604462624\n",
      "Loss =  1.203377508558333\n",
      "Loss =  1.2840706091374159\n",
      "Loss =  1.3730702567845583\n",
      "Loss =  1.508686974644661\n",
      "Loss =  1.6163019593805075\n",
      "Loss =  1.728560090996325\n",
      "Loss =  2.2809643829241395\n",
      "Loss =  3.6867697471752763\n",
      "Loss =  4.794033252634108\n",
      "Loss =  5.084158171899617\n",
      "Loss =  5.1729427659884095\n",
      "Loss =  5.418704395182431\n",
      "Loss =  5.771099385805428\n",
      "Loss =  6.222203214652836\n",
      "Loss =  6.516791035421193\n",
      "Loss =  7.029604044742882\n",
      "Loss =  7.284040053375065\n",
      "Loss =  7.39398870524019\n",
      "Epoch 18 -- loss = 7.541773003526032\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loss =  0.015852438285946846\n",
      "Loss =  0.1611004751175642\n",
      "Loss =  0.3559216968715191\n",
      "Loss =  1.178021814674139\n",
      "Loss =  2.558051463216543\n",
      "Loss =  3.2672074995934963\n",
      "Loss =  3.570243399590254\n",
      "Loss =  3.6765294754877687\n",
      "Loss =  3.7520193615928292\n",
      "Loss =  3.8473061211407185\n",
      "Loss =  3.9312180299311876\n",
      "Loss =  4.0321778282523155\n",
      "Loss =  4.18062749505043\n",
      "Loss =  4.7585125379264355\n",
      "Loss =  5.499220829457045\n",
      "Loss =  6.195820730179548\n",
      "Loss =  6.711502630263567\n",
      "Loss =  7.186696529388428\n",
      "Loss =  7.345417445525527\n",
      "Loss =  7.658962665125728\n",
      "Loss =  8.198924070224166\n",
      "Epoch 19 -- loss = 8.38964600674808\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rnn = my_rnn_1(input_size=sequence_size, hidden_size=hidden_size, num_layers=3, batch_size=batch_size).cuda()\n",
    "demo_target = Variable(torch.randn((batch_size, 1))).cuda()\n",
    "X = Variable(torch.FloatTensor(X)).cuda()\n",
    "y = Variable(torch.FloatTensor(y)).cuda()\n",
    "loss_fn = nn.MSELoss()\n",
    "# optimizer = optim.SGD(rnn.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.RMSprop(rnn.parameters())\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for batch, i in enumerate(range(0, X.size(0) - 1, batch_size)):\n",
    "        hidden = rnn.init_hidden()\n",
    "        data, targets = get_batch(X, y, i)\n",
    "        if data.size(0) < batch_size:\n",
    "            break\n",
    "        output, hidden = rnn(data, hidden)\n",
    "        #print output\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(output, targets)\n",
    "        loss.backward()\n",
    "        # loss.backward(retain_variables=True)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "        if i % 10 == 0:\n",
    "            print(\"Loss = \", total_loss)\n",
    "    print(\"Epoch \" + str(epoch) + \" -- loss = \" + str(total_loss))\n",
    "    print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
